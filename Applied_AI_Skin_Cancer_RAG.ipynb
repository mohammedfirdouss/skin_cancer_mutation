{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9189fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary frameworks\n",
    "!pip install -r requirements.txt\n",
    "# Or install directly if requirements.txt is not present:\n",
    "# !pip install llama-index llama-index-llms-huggingface llama-index-embeddings-huggingface transformers accelerate bitsandbytes datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a0539a",
   "metadata": {},
   "source": [
    "## 1. Data Ingestion & Filtering\n",
    "We reuse the logic to fetch and filter the Mol-Instructions dataset, but we will wrap the output into LlamaIndex `Document` objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baae9ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from llama_index.core import Document\n",
    "from datasets import load_dataset\n",
    "\n",
    "class MolInstructionsLoader:\n",
    "    \"\"\"Loads and filters data, returning LlamaIndex Documents.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir=\"./data\"):\n",
    "        self.cache_dir = Path(cache_dir)\n",
    "        self.cache_dir.mkdir(exist_ok=True)\n",
    "        self.filtered_file = self.cache_dir / \"cancer_filtered.json\"\n",
    "\n",
    "    def load_documents(self, max_samples=2000):\n",
    "        data = self._get_filtered_data(max_samples)\n",
    "        documents = []\n",
    "        \n",
    "        print(\"Converting to LlamaIndex Documents...\")\n",
    "        for item in data:\n",
    "            # We combine instruction, input, and output into the main text\n",
    "            text = f\"Instruction: {item['instruction']}\\nInput: {item['input']}\\nOutput: {item['output']}\"\n",
    "            \n",
    "            # Metadata is crucial for Applied AI (filtering, tracing)\n",
    "            metadata = {\n",
    "                \"source\": \"Mol-Instructions\",\n",
    "                \"id\": item['id'],\n",
    "                \"task\": \"mutation_analysis\"\n",
    "            }\n",
    "            \n",
    "            doc = Document(text=text, metadata=metadata)\n",
    "            documents.append(doc)\n",
    "            \n",
    "        print(f\"Created {len(documents)} documents.\")\n",
    "        return documents\n",
    "\n",
    "    def _get_filtered_data(self, max_samples):\n",
    "        if self.filtered_file.exists():\n",
    "            print(\"Loading cached data...\")\n",
    "            with open(self.filtered_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        \n",
    "        print(\"Downloading and filtering dataset...\")\n",
    "        # (Simplified filtering logic from original notebook)\n",
    "        try:\n",
    "            dataset = load_dataset(\"zjunlp/Mol-Instructions\", \"Molecule-oriented Instructions\", split=\"train\", streaming=True)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset: {e}\")\n",
    "            return []\n",
    "\n",
    "        cancer_keywords = ['cancer', 'tumor', 'mutation', 'melanoma', 'braf', 'tp53', 'v600e']\n",
    "        filtered_data = []\n",
    "        count = 0\n",
    "        \n",
    "        for example in dataset:\n",
    "            if count >= max_samples: break\n",
    "            text = f\"{example.get('instruction', '')} {example.get('output', '')}\".lower()\n",
    "            if any(k in text for k in cancer_keywords):\n",
    "                filtered_data.append({\n",
    "                    'instruction': example.get('instruction', ''),\n",
    "                    'input': example.get('input', ''),\n",
    "                    'output': example.get('output', ''),\n",
    "                    'id': count\n",
    "                })\n",
    "                count += 1\n",
    "        \n",
    "        with open(self.filtered_file, 'w') as f:\n",
    "            json.dump(filtered_data, f)\n",
    "        return filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bd4bbc8",
   "metadata": {},
   "source": [
    "## 2. Setup Embeddings & LLM (The \"Applied\" Stack)\n",
    "We use `HuggingFaceEmbedding` for local embeddings and `HuggingFaceLLM` for the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b816ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.llms.huggingface import HuggingFaceLLM\n",
    "from llama_index.core import Settings\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "# 1. Setup Embeddings\n",
    "print(\"Loading Embeddings...\")\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# 2. Setup Quantized LLM\n",
    "# Note: This requires GPU. If running on CPU only, this cell might fail or be extremely slow.\n",
    "print(\"Loading Quantized LLM...\")\n",
    "\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "llm = HuggingFaceLLM(\n",
    "    model_name=\"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    tokenizer_name=\"unsloth/Llama-3.2-1B-Instruct\",\n",
    "    context_window=2048,\n",
    "    max_new_tokens=256,\n",
    "    model_kwargs={\"quantization_config\": quantization_config},\n",
    "    generate_kwargs={\"temperature\": 0.7, \"do_sample\": True},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 3. Configure Global Settings\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = embed_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5056e3",
   "metadata": {},
   "source": [
    "## 3. Indexing & Retrieval\n",
    "We build a `VectorStoreIndex`. In a production \"Applied AI\" setting, you would persist this to disk (e.g., using ChromaDB) so you don't rebuild it every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b88e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "# Load data\n",
    "loader = MolInstructionsLoader()\n",
    "documents = loader.load_documents(max_samples=500)\n",
    "\n",
    "# Build Index\n",
    "print(\"Building Vector Index...\")\n",
    "index = VectorStoreIndex.from_documents(documents)\n",
    "\n",
    "# Create Query Engine\n",
    "query_engine = index.as_query_engine(similarity_top_k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78f458d",
   "metadata": {},
   "source": [
    "## 4. Running Queries\n",
    "Now we can ask questions. The system will retrieve relevant contexts and generate an answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a15bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = query_engine.query(\"What is the clinical significance of BRAF V600E mutation in melanoma?\")\n",
    "print(\"\\nResponse:\")\n",
    "print(response)\n",
    "\n",
    "# Inspecting the retrieved nodes (Applied AI: Traceability)\n",
    "print(\"\\n--- Source Documents ---\")\n",
    "for node in response.source_nodes:\n",
    "    print(f\"[Score: {node.score:.3f}] {node.text[:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
