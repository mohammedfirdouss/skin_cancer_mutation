{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYmK938MZLKS"
      },
      "outputs": [],
      "source": [
        "# getting all libraries and dependencies ready\n",
        "import warnings\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Optional\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ignoring any upcoming warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "ydjqobGQe1_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STATEMENT: Developing a RAG-powered question-and-answer system that focuses on skin-cancer-related protein mutations.**\n",
        "\n",
        "# This is an effort to close a critical knowledge gap in clinical and molecular oncology within Africa And all over the world. Skin cancer is rising across multiple regions of the continent due to increasing UV exposure, late diagnosis, limited access to dermatological specialists, and insufficient molecular diagnostic resources. Many health centres do not have immediate access to detailed genomic information, and when protein-level mutations are involved, the interpretation requires both specialized knowledge and reliable reference data. A Retrieval-Augmented Generation system provides a way to blend trusted scientific literature, curated genomic databases, and clinical guidelines into a single intelligent assistant that can respond to questions with precision and contextual clarity. The importance of such a system in Africa lies in its ability to reduce barriers to expert knowledge, especially in settings where oncologists, dermatopathologists, or molecular biologists may not be readily available.\n",
        "\n",
        "This project is designed to strengthen clinical decision-support capacity by allowing medical personnel to ask direct questions about mutations associated with melanoma and other skin cancers, understanding how these mutations affect protein structure, disease progression, treatment response, and prognosis. Instead of relying solely on limited local expertise or outdated texts, clinicians can obtain up-to-date and evidence-based answers drawn from high-quality molecular databases, published dermatology research, and recognized cancer-genomics repositories. The RAG architecture ensures that the system retrieves factual information rather than inventing unsupported explanations, making it especially suitable for sensitive medical domains.\n",
        "\n",
        "The solution aims to assist dermatologists who might need rapid clarification on the clinical significance of a mutation detected during biopsy evaluation. It supports pathologists who interpret histology and immunohistochemistry images but need molecular confirmation for difficult cases. It aids oncologists who formulate treatment plans influenced by mutation profiles such as BRAF, NRAS, or KIT alterations. It benefits molecular geneticists who analyse sequencing data in laboratories that often face staffing shortages. It also supports general practitioners operating in remote regions who frequently encounter skin lesions without specialist backup. In teaching hospitals, it provides an additional layer of academic support for medical students, resident doctors, and biomedical scientists learning how molecular mechanisms drive disease outcomes.\n",
        "\n",
        "Ultimately, the system is being developed to expand access to reliable molecular oncology insights across African healthcare environments. Many facilities lack immediate access to advanced diagnostic equipment or specialists trained in protein mutation interpretation. A RAG-based assistant reduces this gap by delivering accurate, context-aware explanations that can guide patient triage, improve diagnostic confidence, and help clinicians make more informed decisions. It does not replace medical professionals but strengthens their capacity to deliver better care through improved access to molecular knowledge that is often scarce in the region. This project represents a step towards democratizing precision-medicine-level information for the African medical landscape, where timely understanding of protein mutations in skin cancer can significantly influence patient outcomes.\n"
      ],
      "metadata": {
        "id": "jFaAVli9ajqP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# installing the lib for quantizing,transforming,LLm to fast inference,response and manage memory\n",
        "!pip install -q datasets transformers sentence-transformers faiss-cpu\n",
        "!pip install -q bitsandbytes accelerate\n",
        "!pip install -q gradio requests"
      ],
      "metadata": {
        "id": "H-4wu7suS92o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preparation and filtering\n",
        "\n",
        "class MolInstructionsFilter:\n",
        "    \"\"\"class to Filter and prepare cancer-related data from\n",
        "     Mol-Instructions using the feature sets of skin-cancer-related proteins.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cache_dir=\"./data\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.filtered_file = self.cache_dir / \"cancer_filtered.json\"\n",
        "\n",
        "    def download_and_filter(self, max_samples=5000):\n",
        "        \"\"\"Downloading and filtering relevant cancer mutation data\"\"\"\n",
        "\n",
        "        from datasets import load_dataset\n",
        "\n",
        "        if self.filtered_file.exists():\n",
        "            print(\"Filtered cancer mutation data already exists. Loading from cache...\")\n",
        "            with open(self.filtered_file, 'r') as f:\n",
        "                return json.load(f)\n",
        "\n",
        "        print(\"Downloading Mol-Instructions dataset...\")\n",
        "\n",
        "        # Loading the specific subsets that are relevant to mutation\n",
        "        # Focusing on description and property prediction tasks\n",
        "        try:\n",
        "            dataset = load_dataset(\n",
        "                \"zjunlp/Mol-Instructions\",\n",
        "                \"Molecule-oriented Instructions\",\n",
        "                split=\"train\",\n",
        "                streaming=True  # I'm Streaming to avoid loading entire 1B params\n",
        "            )\n",
        "        except:\n",
        "            return []\n",
        "\n",
        "        # Keywords for filtering skin cancer-related content\n",
        "        cancer_keywords = [\n",
        "            'cancer', 'tumor', 'mutation', 'oncology', 'melanoma',\n",
        "            'carcinoma', 'metastasis', 'oncogene', 'braf', 'tp53',\n",
        "            'egfr', 'kras', 'protein', 'inhibitor', 'therapy',\n",
        "            'V600E','NRAS mutation'\n",
        "            ]\n",
        "\n",
        "        filtered_data = []\n",
        "        count = 0\n",
        "\n",
        "        print(\"Filtering for cancer-related content...\")\n",
        "        for example in dataset:\n",
        "            if count >= max_samples:\n",
        "                break\n",
        "\n",
        "            #  Checking if output contains cancer keywords\n",
        "            text = f\"{example.get('instruction', '')} {example.get('output', '')}\".lower()\n",
        "\n",
        "            if any(keyword in text for keyword in cancer_keywords):\n",
        "                filtered_data.append({\n",
        "                    'instruction': example.get('instruction', ''),\n",
        "                    'input': example.get('input', ''),\n",
        "                    'output': example.get('output', ''),\n",
        "                    'id': count\n",
        "                })\n",
        "                count += 1\n",
        "\n",
        "                if count % 100 == 0:\n",
        "                    print(f\"Collected {count} samples...\")\n",
        "\n",
        "        # Saving filtered data\n",
        "        with open(self.filtered_file, 'w') as f:\n",
        "            json.dump(filtered_data, f, indent=2)\n",
        "\n",
        "        print(f\"Filtered {len(filtered_data)} cancer-related samples\")\n",
        "        return filtered_data\n"
      ],
      "metadata": {
        "id": "G_aqcliCdJJh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# inspecting the path of my filtered data\n",
        "# obj = MolInstructionsFilter()\n",
        "# print(obj.cache_dir)\n",
        "# print(obj.filtered_file)"
      ],
      "metadata": {
        "id": "sWXNzLkclznD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# trying to get an overview of the downloaded-json file\n",
        "# json_obj = MolInstructionsFilter()\n",
        "# data = json_obj.download_and_filter()\n",
        "# print(data)"
      ],
      "metadata": {
        "id": "ERwx3CUYm-j9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mf = MolInstructionsFilter()\n",
        "print(mf)                 # printing the filtered object reference\n",
        "print(mf.cache_dir)       # printing the data directory\n",
        "print(mf.filtered_file)   # printing data/cancer_filtered.json loc"
      ],
      "metadata": {
        "id": "mumdsvFVoT8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**KNOWLEDGE BASE DESIGN/CONNECTIVITY \" UNIPROT \"**"
      ],
      "metadata": {
        "id": "_8_hO6a_UWBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UniProtCache:\n",
        "    \"\"\"Creating Cached access to UniProt protein database\"\"\"\n",
        "\n",
        "    def __init__(self, cache_dir=\"./data\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.cache_file = self.cache_dir / \"uniprot_cache.json\"\n",
        "        self.cache = self._load_cache()\n",
        "\n",
        "        # Listing some common skin cancer proteins\n",
        "        self.cancer_proteins = [\n",
        "            'BRAF', 'TP53', 'NRAS', 'CDKN2A', 'PTEN',\n",
        "            'KIT', 'NF1', 'MAP2K1', 'TERT', 'ARID2'\n",
        "        ]\n",
        "\n",
        "    def _load_cache(self):\n",
        "        if self.cache_file.exists():\n",
        "            with open(self.cache_file, 'r') as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def _save_cache(self):\n",
        "        with open(self.cache_file, 'w') as f:\n",
        "            json.dump(self.cache, f, indent=2)\n",
        "\n",
        "    def fetch_protein_info(self, gene_name: str) -> Optional[Dict]:\n",
        "        \"\"\"Fetch protein info from UniProt (cached)\"\"\"\n",
        "        if gene_name in self.cache:\n",
        "            return self.cache[gene_name]\n",
        "\n",
        "        try:\n",
        "            # APi request to extract the protein features of skin cancer\n",
        "            url = f\"https://rest.uniprot.org/uniprotkb/search?query=gene:{gene_name}+AND+organism_id:9606&format=json&size=1\"\n",
        "            response = requests.get(url, timeout=10)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                if data.get('results'):\n",
        "                    result = data['results'][0]\n",
        "                    info = {\n",
        "                        'gene': gene_name,\n",
        "                        'protein_name': result.get('proteinDescription', {}).get('recommendedName', {}).get('fullName', {}).get('value', 'Unknown'),\n",
        "                        'function': result.get('comments', [{}])[0].get('texts', [{}])[0].get('value', 'No function info'),\n",
        "                        'accession': result.get('primaryAccession', ''),\n",
        "                        'sequence_length': result.get('sequence', {}).get('length', 0)\n",
        "                    }\n",
        "                    self.cache[gene_name] = info\n",
        "                    self._save_cache()\n",
        "                    return info\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching {gene_name}: {e}\")\n",
        "\n",
        "        return None\n",
        "\n",
        "    def preload_cancer_proteins(self):\n",
        "        \"\"\"Preloading the common skin cancer proteins\"\"\"\n",
        "        print(\"Preloading cancer protein database...\")\n",
        "        for protein in self.cancer_proteins:\n",
        "            if protein not in self.cache:\n",
        "                print(f\"  Fetching {protein}...\")\n",
        "                self.fetch_protein_info(protein)\n",
        "        print(f\"Cached {len(self.cache)} proteins\")\n"
      ],
      "metadata": {
        "id": "CRKX-G9-oTr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAG PIPPELINE SETUP: EMBEDDING,RETRIEVER, AUGMENTATIONS**"
      ],
      "metadata": {
        "id": "513zk6dcoBMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CancerRAGRetriever:\n",
        "    \"\"\"Employing efficient retrieval system using FAISS\"\"\"\n",
        "\n",
        "    def __init__(self, cache_dir=\"./data\"):\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.index_file = self.cache_dir / \"faiss_index.bin\"\n",
        "        self.docs_file = self.cache_dir / \"documents.pkl\"\n",
        "\n",
        "        # Loading a lightweight embedding model using sentence transformer\n",
        "        # To avoid memory overuse on my env\n",
        "        from sentence_transformers import SentenceTransformer\n",
        "        print(\"Loading embedding model...\")\n",
        "        self.embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        print(\"Embedding model loaded successfully\")\n",
        "\n",
        "        self.index = None\n",
        "        self.documents = []\n",
        "\n",
        "    def build_index(self, data: List[Dict]):\n",
        "        \"\"\"Building the FAISS index from filtered data\"\"\"\n",
        "        import faiss\n",
        "\n",
        "        if self.index_file.exists() and self.docs_file.exists():\n",
        "            print(\"Loading existing FAISS index...\")\n",
        "            self.index = faiss.read_index(str(self.index_file))\n",
        "            with open(self.docs_file, 'rb') as f:\n",
        "                self.documents = pickle.load(f)\n",
        "            print(f\"Loaded index with {len(self.documents)} documents\")\n",
        "            return\n",
        "\n",
        "        print(\"Building FAISS index from scratch...\")\n",
        "\n",
        "        # Preparing documents for indexing\n",
        "        self.documents = []\n",
        "        texts_to_embed = []\n",
        "\n",
        "        for item in data:\n",
        "            doc_text = f\"{item['instruction']} {item['input']} {item['output']}\"\n",
        "            self.documents.append({\n",
        "                'text': doc_text,\n",
        "                'instruction': item['instruction'],\n",
        "                'output': item['output'],\n",
        "                'id': item['id']\n",
        "            })\n",
        "            texts_to_embed.append(doc_text)\n",
        "\n",
        "        # Now Generating the embeddings\n",
        "        print(\"Generating embeddings...\")\n",
        "        embeddings = self.embedder.encode(\n",
        "            texts_to_embed,\n",
        "            show_progress_bar=True,\n",
        "            batch_size=32\n",
        "        )\n",
        "\n",
        "        # Creating FAISS index\n",
        "        dimension = embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dimension)  # Inner product for cosine similarity(comparative correlation)\n",
        "\n",
        "        # Normalizing the embedings for cosine similarity\n",
        "        faiss.normalize_L2(embeddings)\n",
        "        self.index.add(embeddings)\n",
        "\n",
        "        # Saving the index in pickle file\n",
        "        faiss.write_index(self.index, str(self.index_file))\n",
        "        with open(self.docs_file, 'wb') as f:\n",
        "            pickle.dump(self.documents, f)\n",
        "\n",
        "        print(f\"The index has been built and saved with {len(self.documents)} documents\")\n",
        "\n",
        "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:\n",
        "        \"\"\"Function to retrieve top-k relevant documents\"\"\"\n",
        "        import faiss\n",
        "\n",
        "        # Embed query\n",
        "        query_embedding = self.embedder.encode([query])\n",
        "        faiss.normalize_L2(query_embedding)\n",
        "\n",
        "        # Search\n",
        "        scores, indices = self.index.search(query_embedding, top_k)\n",
        "\n",
        "        # Return results\n",
        "        results = []\n",
        "        for score, idx in zip(scores[0], indices[0]):\n",
        "            if idx < len(self.documents):\n",
        "                results.append({\n",
        "                    'score': float(score),\n",
        "                    'document': self.documents[idx]\n",
        "                })\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "c-4DVJLZYEO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLM GENERATION INT THE QUANTIZED MODEL**"
      ],
      "metadata": {
        "id": "jqXOgeorsZbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QuantizedLLM:\n",
        "    \"\"\"Creating a limited memory of 4-bit quantized\n",
        "      LLM for efficient inference\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name=\"unsloth/Llama-3.2-1B-Instruct\"):\n",
        "        \"\"\"\n",
        "        I'm using the model because of the memory constraints\n",
        "        unsloth/Llama-3.2-1B-Instruct is smallest and uses 1GB of VRAM\n",
        "        which is enough for my application\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "\n",
        "    def load_model(self):\n",
        "        \"\"\"Loading quantized model\"\"\"\n",
        "        if self.model is not None:\n",
        "            return\n",
        "\n",
        "        print(f\"Loading quantized LLM: {self.model_name}...\")\n",
        "\n",
        "        # importing the tokenizer, pytorch model and quantization config to load the model\n",
        "        from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "        import torch\n",
        "\n",
        "        #4-bit quantization configuration\n",
        "        bnb_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,\n",
        "            bnb_4bit_quant_type=\"nf4\",\n",
        "            bnb_4bit_compute_dtype=torch.float16,\n",
        "            bnb_4bit_use_double_quant=True,\n",
        "        )\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            self.model_name,\n",
        "            quantization_config=bnb_config,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        print(\"Model loaded successfully\")\n",
        "\n",
        "    def generate(self, prompt: str, max_length: int = 512) -> str:\n",
        "        \"\"\"Generating response\"\"\"\n",
        "        if self.model is None:\n",
        "            self.load_model()\n",
        "\n",
        "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
        "\n",
        "        outputs = self.model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_length,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            top_p=0.9,\n",
        "            pad_token_id=self.tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extracting only the generated part of the response ( but removed prompt)\n",
        "        response = response[len(prompt):].strip()\n",
        "\n",
        "        return response"
      ],
      "metadata": {
        "id": "IydgQ2YRsFlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAIN RAG PIPELINE:promptly Structured**"
      ],
      "metadata": {
        "id": "x1LWjjZdv4nl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CancerMutationRAG:\n",
        "    \"\"\"Complete RAG pipeline for cancer mutation analysis\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.data_filter = MolInstructionsFilter()\n",
        "        self.uniprot = UniProtCache()\n",
        "        self.retriever = CancerRAGRetriever()\n",
        "        self.llm = QuantizedLLM()\n",
        "        self.initialized = False\n",
        "\n",
        "    def initialize(self):\n",
        "        \"\"\"Initializing all components\"\"\"\n",
        "        if self.initialized:\n",
        "            return\n",
        "\n",
        "        print(\"=\" * 70)\n",
        "        print(\"INITIALIZING CANCER MUTATION RAG SYSTEM\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        # Filtering the dataset\n",
        "        filtered_data = self.data_filter.download_and_filter(max_samples=2000)\n",
        "\n",
        "        # Preloading the proteins\n",
        "        self.uniprot.preload_cancer_proteins()\n",
        "\n",
        "        # Building the retrieval index\n",
        "        self.retriever.build_index(filtered_data)\n",
        "\n",
        "        #Loading LLM (deferred until first query for memory efficiency)\n",
        "        print(\"\\nSystem initializing!\")\n",
        "        print(\"=\" * 70)\n",
        "\n",
        "        self.initialized = True\n",
        "\n",
        "    def query(self, question: str) -> Dict:\n",
        "        \"\"\"Processing a query through the RAG pipeline\"\"\"\n",
        "        if not self.initialized:\n",
        "            self.initialize()\n",
        "\n",
        "        print(f\"\\Processing query: {question}\")\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        print(\"  → Retrieving relevant documents...\")\n",
        "        retrieved_docs = self.retriever.retrieve(question, top_k=3)\n",
        "\n",
        "        #Extracting proteins mentioned and get UniProt data\n",
        "        print(\" Fetching protein information...\")\n",
        "        proteins_mentioned = self._extract_proteins(question)\n",
        "        protein_info = []\n",
        "        for protein in proteins_mentioned:\n",
        "            info = self.uniprot.fetch_protein_info(protein)\n",
        "            if info:\n",
        "                protein_info.append(info)\n",
        "\n",
        "        #Building the RAG context\n",
        "        context = self._build_context(retrieved_docs, protein_info)\n",
        "\n",
        "        #Generate answer of the query\n",
        "        print(\"  → Generating answer with LLM...\")\n",
        "        prompt = self._build_prompt(question, context)\n",
        "        answer = self.llm.generate(prompt, max_length=300)\n",
        "\n",
        "        return {\n",
        "            'question': question,\n",
        "            'answer': answer,\n",
        "            'retrieved_docs': retrieved_docs,\n",
        "            'protein_info': protein_info,\n",
        "            'context': context\n",
        "        }\n",
        "\n",
        "    def _extract_proteins(self, text: str) -> List[str]:\n",
        "        \"\"\"Extracting protein names from text\"\"\"\n",
        "        common_proteins = [\n",
        "            'BRAF', 'TP53', 'NRAS', 'CDKN2A', 'PTEN', 'KIT',\n",
        "            'NF1', 'MAP2K1', 'EGFR', 'KRAS', 'TERT'\n",
        "        ]\n",
        "        text_upper = text.upper()\n",
        "        return [p for p in common_proteins if p in text_upper]\n",
        "\n",
        "    def _build_context(self, docs: List[Dict], proteins: List[Dict]) -> str:\n",
        "        \"\"\"Combining retrieved documents and protein info\"\"\"\n",
        "        context_parts = []\n",
        "\n",
        "        # Adding the retrieved documents\n",
        "        if docs:\n",
        "            context_parts.append(\"Relevant Information:\")\n",
        "            for i, doc in enumerate(docs[:2], 1):  # Top 2 docs\n",
        "                context_parts.append(f\"{i}. {doc['document']['output'][:300]}...\")\n",
        "\n",
        "        # Adding the protein info to contextualize it\n",
        "        if proteins:\n",
        "            context_parts.append(\"\\nProtein Database Information:\")\n",
        "            for protein in proteins:\n",
        "                context_parts.append(\n",
        "                    f\"- {protein['gene']}: {protein['protein_name']}\\n\"\n",
        "                    f\"  Function: {protein['function'][:200]}...\"\n",
        "                )\n",
        "\n",
        "        return \"\\n\".join(context_parts)\n",
        "\n",
        "    def _build_prompt(self, question: str, context: str) -> str:\n",
        "        \"\"\"Build prompt for LLM\"\"\"\n",
        "        return f\"\"\"You are an expert in cancer biology and protein mutations. Answer the question based on the provided context.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer (be concise and scientific):\"\"\"\n"
      ],
      "metadata": {
        "id": "Um7wc_U7sGHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = CancerMutationRAG()\n",
        "print(res._build_prompt)"
      ],
      "metadata": {
        "id": "LYZ9doUkvz5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git"
      ],
      "metadata": {
        "id": "UU9kzEFgKXw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"akintoyesylvester1996@gmail.com\"\n",
        "!git config --global user.name \"Akintoyefelix\""
      ],
      "metadata": {
        "id": "LoFvkOIuKXia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DWn0ZvwA4FkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive/Colab Notebooks\""
      ],
      "metadata": {
        "id": "imlwAQYa4cdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Akintoyefelix/cancer_mutation"
      ],
      "metadata": {
        "id": "MeYu-17HyG8C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/skin_cancer_mutation.ipynb /content/cancer_mutation"
      ],
      "metadata": {
        "id": "UtG9Mya3yjvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/cancer_mutation"
      ],
      "metadata": {
        "id": "HBDTzWhXzT7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git add ."
      ],
      "metadata": {
        "id": "82U51xSm1HrX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"sec_commit skincancer mutattion\""
      ],
      "metadata": {
        "id": "YwsExHIP6O9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote set-url origin https://Akintoyefelix:$token@github.com/Akintoyefelix/cancer_mutation.git"
      ],
      "metadata": {
        "id": "C695jUVaIgD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git push origin main"
      ],
      "metadata": {
        "id": "tHfUu9NPJUxo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git push https://Akintoyefelix:token@github.com/Akintoyefelix/cancer_mutation.git"
      ],
      "metadata": {
        "id": "36cPE43N8Hu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -R \"ghp_\" -n"
      ],
      "metadata": {
        "id": "a90DsJjCAVmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git commit -m \"Commiting Skin Cancer Q%A system \""
      ],
      "metadata": {
        "id": "uElPUNoG6Ojl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!grep -n \"token\" skin_cancer_mutation.ipynb"
      ],
      "metadata": {
        "id": "iSr4_2TcFdFa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}